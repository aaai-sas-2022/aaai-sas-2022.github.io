/*!

=========================================================
* Now UI Kit PRO React - v1.0.0
=========================================================

* Product Page: https://www.creative-tim.com/product/now-ui-kit-pro-react
* Copyright 2020 Creative Tim (http://www.creative-tim.com)

* Coded by Creative Tim

=========================================================

* The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

*/
(this["webpackJsonpmaterial-kit-react"]=this["webpackJsonpmaterial-kit-react"]||[]).push([[0],{100:function(e,a,t){},101:function(e,a,t){},133:function(e,a,t){e.exports=t.p+"static/media/location.2ab7ff0a.jpg"},134:function(e,a,t){"use strict";t.r(a);t(100);var n=t(20),r=(t(101),t(0)),i=t.n(r),o=t(12),l=t.n(o),s=t(11),c=t(168),p=t(6),m=Object(p.a)(Object(p.a)({},{paddingRight:"15px",paddingLeft:"15px",marginRight:"auto",marginLeft:"auto",width:"100%"}),{},{"@media (min-width: 576px)":{maxWidth:"540px"},"@media (min-width: 768px)":{maxWidth:"720px"},"@media (min-width: 992px)":{maxWidth:"960px"},"@media (min-width: 1200px)":{maxWidth:"1140px"}}),d={fontFamily:'"Roboto", "Helvetica", "Arial", sans-serif',fontWeight:"300",lineHeight:"1.5em"},g=(Object(p.a)({color:"#fff",background:"linear-gradient(60deg, #ffa726, #fb8c00)"},{boxShadow:"0 12px 20px -10px rgba(255, 152, 0, 0.28), 0 4px 20px 0px rgba(0, 0, 0, 0.12), 0 7px 8px -5px rgba(255, 152, 0, 0.2)"}),Object(p.a)({color:"#fff",background:"linear-gradient(60deg, #66bb6a, #43a047)"},{boxShadow:"0 12px 20px -10px rgba(76, 175, 80, 0.28), 0 4px 20px 0px rgba(0, 0, 0, 0.12), 0 7px 8px -5px rgba(76, 175, 80, 0.2)"}),Object(p.a)({color:"#fff",background:"linear-gradient(60deg, #ef5350, #e53935)"},{boxShadow:"0 12px 20px -10px rgba(244, 67, 54, 0.28), 0 4px 20px 0px rgba(0, 0, 0, 0.12), 0 7px 8px -5px rgba(244, 67, 54, 0.2)"}),Object(p.a)({color:"#fff",background:"linear-gradient(60deg, #26c6da, #00acc1)"},{boxShadow:"0 12px 20px -10px rgba(0, 188, 212, 0.28), 0 4px 20px 0px rgba(0, 0, 0, 0.12), 0 7px 8px -5px rgba(0, 188, 212, 0.2)"}),Object(p.a)({color:"#fff",background:"linear-gradient(60deg, #ab47bc, #8e24aa)"},{boxShadow:"0 12px 20px -10px rgba(156, 39, 176, 0.28), 0 4px 20px 0px rgba(0, 0, 0, 0.12), 0 7px 8px -5px rgba(156, 39, 176, 0.2)"}),Object(p.a)({color:"#fff",background:"linear-gradient(60deg, #ec407a, #d81b60)"},{boxShadow:"0 4px 20px 0px rgba(0, 0, 0, 0.14), 0 7px 10px -5px rgba(233, 30, 99, 0.4)"}),Object(p.a)({margin:"0 20px 10px",paddingTop:"10px",borderTop:"1px solid #eeeeee",height:"auto"},d),{color:"#3C4858",margin:"1.75rem 0 0.875rem",textDecoration:"none",fontWeight:"700",fontFamily:'"Roboto Slab", "Times New Roman", serif'}),h=Object(p.a)(Object(p.a)({},g),{},{marginTop:".625rem"}),u={container:Object(p.a)({zIndex:"12",color:"#FFFFFF"},m),title:Object(p.a)(Object(p.a)({},g),{},{fontSize:"2.5em",display:"inline-block",position:"relative",minHeight:"32px",maxWidth:"600px",marginTop:"0px",marginBottom:"30px",color:"#FFFFFF",textDecoration:"none"}),subtitle:{marginTop:"30px",fontSize:"2rem"},main:{background:"#FFFFFF",position:"relative",zIndex:"3"},mainRaised:{margin:"-60px 3% 0px",borderRadius:"6px",boxShadow:"0 16px 24px 2px rgba(0, 0, 0, 0.14), 0 6px 30px 5px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(0, 0, 0, 0.2)"}},x=t(19),b=t.n(x),f=t(7),E=t(29),v=t(82),w=t.n(v),S=t(167),k={button:{minHeight:"auto",minWidth:"auto",backgroundColor:"#999999",color:"#FFFFFF",boxShadow:"0 2px 2px 0 rgba(153, 153, 153, 0.14), 0 3px 1px -2px rgba(153, 153, 153, 0.2), 0 1px 5px 0 rgba(153, 153, 153, 0.12)",border:"none",borderRadius:"3px",position:"relative",padding:"12px 30px",margin:".3125rem 1px",fontSize:"12px",fontWeight:"400",textTransform:"uppercase",letterSpacing:"0",willChange:"box-shadow, transform",transition:"box-shadow 0.2s cubic-bezier(0.4, 0, 1, 1), background-color 0.2s cubic-bezier(0.4, 0, 0.2, 1)",lineHeight:"1.42857143",textAlign:"center",whiteSpace:"nowrap",verticalAlign:"middle",touchAction:"manipulation",cursor:"pointer","&:hover,&:focus":{color:"#FFFFFF",backgroundColor:"#999999",boxShadow:"0 14px 26px -12px rgba(153, 153, 153, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(153, 153, 153, 0.2)"},"& .fab,& .fas,& .far,& .fal,& .material-icons":{position:"relative",display:"inline-block",top:"0",fontSize:"1.1rem",marginRight:"4px",verticalAlign:"middle"},"& svg":{position:"relative",display:"inline-block",top:"0",width:"18px",height:"18px",marginRight:"4px",verticalAlign:"middle"},"&$justIcon":{"& .fab,& .fas,& .far,& .fal,& .material-icons":{marginRight:"0px",position:"absolute",width:"100%",transform:"none",left:"0px",top:"0px",height:"100%",lineHeight:"41px",fontSize:"20px"}}},fullWidth:{width:"100%"},primary:{backgroundColor:"#9c27b0",boxShadow:"0 2px 2px 0 rgba(156, 39, 176, 0.14), 0 3px 1px -2px rgba(156, 39, 176, 0.2), 0 1px 5px 0 rgba(156, 39, 176, 0.12)","&:hover,&:focus":{backgroundColor:"#9c27b0",boxShadow:"0 14px 26px -12px rgba(156, 39, 176, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(156, 39, 176, 0.2)"}},info:{backgroundColor:"#00acc1",boxShadow:"0 2px 2px 0 rgba(0, 188, 212, 0.14), 0 3px 1px -2px rgba(0, 188, 212, 0.2), 0 1px 5px 0 rgba(0, 188, 212, 0.12)","&:hover,&:focus":{backgroundColor:"#00acc1",boxShadow:"0 14px 26px -12px rgba(0, 188, 212, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(0, 188, 212, 0.2)"}},success:{backgroundColor:"#4caf50",boxShadow:"0 2px 2px 0 rgba(76, 175, 80, 0.14), 0 3px 1px -2px rgba(76, 175, 80, 0.2), 0 1px 5px 0 rgba(76, 175, 80, 0.12)","&:hover,&:focus":{backgroundColor:"#4caf50",boxShadow:"0 14px 26px -12px rgba(76, 175, 80, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(76, 175, 80, 0.2)"}},warning:{backgroundColor:"#ff9800",boxShadow:"0 2px 2px 0 rgba(255, 152, 0, 0.14), 0 3px 1px -2px rgba(255, 152, 0, 0.2), 0 1px 5px 0 rgba(255, 152, 0, 0.12)","&:hover,&:focus":{backgroundColor:"#ff9800",boxShadow:"0 14px 26px -12px rgba(255, 152, 0, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(255, 152, 0, 0.2)"}},danger:{backgroundColor:"#f44336",boxShadow:"0 2px 2px 0 rgba(244, 67, 54, 0.14), 0 3px 1px -2px rgba(244, 67, 54, 0.2), 0 1px 5px 0 rgba(244, 67, 54, 0.12)","&:hover,&:focus":{backgroundColor:"#f44336",boxShadow:"0 14px 26px -12px rgba(244, 67, 54, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(244, 67, 54, 0.2)"}},rose:{backgroundColor:"#e91e63",boxShadow:"0 2px 2px 0 rgba(233, 30, 99, 0.14), 0 3px 1px -2px rgba(233, 30, 99, 0.2), 0 1px 5px 0 rgba(233, 30, 99, 0.12)","&:hover,&:focus":{backgroundColor:"#e91e63",boxShadow:"0 14px 26px -12px rgba(233, 30, 99, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(233, 30, 99, 0.2)"}},white:{"&,&:focus,&:hover,&:visited":{backgroundColor:"#FFFFFF",color:"#999999"}},twitter:{backgroundColor:"#55acee",color:"#fff",boxShadow:"0 2px 2px 0 rgba(85, 172, 238, 0.14), 0 3px 1px -2px rgba(85, 172, 238, 0.2), 0 1px 5px 0 rgba(85, 172, 238, 0.12)","&:hover,&:focus,&:visited":{backgroundColor:"#55acee",color:"#fff",boxShadow:"0 14px 26px -12px rgba(85, 172, 238, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(85, 172, 238, 0.2)"}},facebook:{backgroundColor:"#3b5998",color:"#fff",boxShadow:"0 2px 2px 0 rgba(59, 89, 152, 0.14), 0 3px 1px -2px rgba(59, 89, 152, 0.2), 0 1px 5px 0 rgba(59, 89, 152, 0.12)","&:hover,&:focus":{backgroundColor:"#3b5998",color:"#fff",boxShadow:"0 14px 26px -12px rgba(59, 89, 152, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(59, 89, 152, 0.2)"}},google:{backgroundColor:"#dd4b39",color:"#fff",boxShadow:"0 2px 2px 0 rgba(221, 75, 57, 0.14), 0 3px 1px -2px rgba(221, 75, 57, 0.2), 0 1px 5px 0 rgba(221, 75, 57, 0.12)","&:hover,&:focus":{backgroundColor:"#dd4b39",color:"#fff",boxShadow:"0 14px 26px -12px rgba(221, 75, 57, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(221, 75, 57, 0.2)"}},github:{backgroundColor:"#333333",color:"#fff",boxShadow:"0 2px 2px 0 rgba(51, 51, 51, 0.14), 0 3px 1px -2px rgba(51, 51, 51, 0.2), 0 1px 5px 0 rgba(51, 51, 51, 0.12)","&:hover,&:focus":{backgroundColor:"#333333",color:"#fff",boxShadow:"0 14px 26px -12px rgba(51, 51, 51, 0.42), 0 4px 23px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(51, 51, 51, 0.2)"}},simple:{"&,&:focus,&:hover,&:visited":{color:"#FFFFFF",background:"transparent",boxShadow:"none"},"&$primary":{"&,&:focus,&:hover,&:visited":{color:"#9c27b0"}},"&$info":{"&,&:focus,&:hover,&:visited":{color:"#00acc1"}},"&$success":{"&,&:focus,&:hover,&:visited":{color:"#4caf50"}},"&$warning":{"&,&:focus,&:hover,&:visited":{color:"#ff9800"}},"&$rose":{"&,&:focus,&:hover,&:visited":{color:"#e91e63"}},"&$danger":{"&,&:focus,&:hover,&:visited":{color:"#f44336"}},"&$twitter":{"&,&:focus,&:hover,&:visited":{color:"#55acee"}},"&$facebook":{"&,&:focus,&:hover,&:visited":{color:"#3b5998"}},"&$google":{"&,&:focus,&:hover,&:visited":{color:"#dd4b39"}},"&$github":{"&,&:focus,&:hover,&:visited":{color:"#333333"}}},transparent:{"&,&:focus,&:hover,&:visited":{color:"inherit",background:"transparent",boxShadow:"none"}},disabled:{opacity:"0.65",pointerEvents:"none"},lg:{padding:"1.125rem 2.25rem",fontSize:"0.875rem",lineHeight:"1.333333",borderRadius:"0.2rem"},sm:{padding:"0.40625rem 1.25rem",fontSize:"0.6875rem",lineHeight:"1.5",borderRadius:"0.2rem"},round:{borderRadius:"30px"},block:{width:"100% !important"},link:{"&,&:hover,&:focus":{backgroundColor:"transparent",color:"#999999",boxShadow:"none"}},justIcon:{paddingLeft:"12px",paddingRight:"12px",fontSize:"20px",height:"41px",minWidth:"41px",width:"41px","& .fab,& .fas,& .far,& .fal,& svg,& .material-icons":{marginRight:"0px"},"&$lg":{height:"57px",minWidth:"57px",width:"57px",lineHeight:"56px","& .fab,& .fas,& .far,& .fal,& .material-icons":{fontSize:"32px",lineHeight:"56px"},"& svg":{width:"32px",height:"32px"}},"&$sm":{height:"30px",minWidth:"30px",width:"30px","& .fab,& .fas,& .far,& .fal,& .material-icons":{fontSize:"17px",lineHeight:"29px"},"& svg":{width:"17px",height:"17px"}}}},y=["color","round","children","fullWidth","disabled","simple","size","block","link","justIcon","className"],j=w()((function(){return Object(p.a)({},k)})),A=i.a.forwardRef((function(e,a){var t,n=e.color,r=e.round,o=e.children,l=e.fullWidth,s=e.disabled,c=e.simple,p=e.size,m=e.block,d=e.link,g=e.justIcon,h=e.className,u=Object(E.a)(e,y),x=j(),v=b()((t={},Object(f.a)(t,x.button,!0),Object(f.a)(t,x[p],p),Object(f.a)(t,x[n],n),Object(f.a)(t,x.round,r),Object(f.a)(t,x.fullWidth,l),Object(f.a)(t,x.disabled,s),Object(f.a)(t,x.simple,c),Object(f.a)(t,x.block,m),Object(f.a)(t,x.link,d),Object(f.a)(t,x.justIcon,g),Object(f.a)(t,h,h),t));return i.a.createElement(S.a,Object.assign({},u,{ref:a,className:v}),o)})),N=t(169),F=["children","className"],C=Object(c.a)({grid:{marginRight:"-15px",marginLeft:"-15px",width:"auto"}});function O(e){var a=C(),t=e.children,n=e.className,r=Object(E.a)(e,F);return i.a.createElement(N.a,Object.assign({container:!0},r,{className:a.grid+" "+n}),t)}O.defaultProps={className:""};var T=["children","className"],L=Object(c.a)({grid:{position:"relative",width:"100%",minHeight:"1px",paddingRight:"15px",paddingLeft:"15px",flexBasis:"auto"}});function I(e){var a=L(),t=e.children,n=e.className,r=Object(E.a)(e,T);return i.a.createElement(N.a,Object.assign({item:!0},r,{className:a.grid+" "+n}),t)}I.defaultProps={className:""};var R=t(43),P=t(170),H=t(171),B=t(172),z=t(185),D=t(186),W=t(87),M=t.n(W),U=t(25),_={appBar:{display:"flex",border:"0",borderRadius:"3px",padding:"0.625rem 0",marginBottom:"20px",color:"#555",width:"100%",backgroundColor:"#fff",boxShadow:"0 4px 18px 0px rgba(0, 0, 0, 0.12), 0 7px 10px -5px rgba(0, 0, 0, 0.15)",transition:"all 150ms ease 0s",alignItems:"center",flexFlow:"row nowrap",justifyContent:"flex-start",position:"relative",zIndex:"unset"},absolute:{position:"absolute",zIndex:"1100"},fixed:{position:"fixed",zIndex:"1100"},container:Object(p.a)(Object(p.a)({},m),{},{minHeight:"50px",flex:"1",alignItems:"center",justifyContent:"space-between",display:"flex",flexWrap:"nowrap"}),flex:{flex:1},title:Object(p.a)(Object(p.a)({},d),{},{lineHeight:"30px",fontSize:"18px",borderRadius:"3px",textTransform:"none",color:"inherit",padding:"8px 16px",letterSpacing:"unset","&:hover,&:focus":{color:"inherit",background:"transparent"}}),appResponsive:{margin:"20px 10px"},primary:{backgroundColor:"#9c27b0",color:"#FFFFFF",boxShadow:"0 4px 20px 0px rgba(0, 0, 0, 0.14), 0 7px 12px -5px rgba(156, 39, 176, 0.46)"},info:{backgroundColor:"#00acc1",color:"#FFFFFF",boxShadow:"0 4px 20px 0px rgba(0, 0, 0, 0.14), 0 7px 12px -5px rgba(0, 188, 212, 0.46)"},success:{backgroundColor:"#4caf50",color:"#FFFFFF",boxShadow:"0 4px 20px 0px rgba(0, 0, 0, 0.14), 0 7px 12px -5px rgba(76, 175, 80, 0.46)"},warning:{backgroundColor:"#ff9800",color:"#FFFFFF",boxShadow:"0 4px 20px 0px rgba(0, 0, 0, 0.14), 0 7px 12px -5px rgba(255, 152, 0, 0.46)"},danger:{backgroundColor:"#f44336",color:"#FFFFFF",boxShadow:"0 4px 20px 0px rgba(0, 0, 0, 0.14), 0 7px 12px -5px rgba(244, 67, 54, 0.46)"},rose:{backgroundColor:"#e91e63",color:"#FFFFFF",boxShadow:"0 4px 20px 0px rgba(0, 0, 0, 0.14), 0 7px 12px -5px rgba(233, 30, 99, 0.46)"},transparent:{backgroundColor:"transparent !important",boxShadow:"none",paddingTop:"25px",color:"#FFFFFF"},dark:{color:"#FFFFFF",backgroundColor:"#212121 !important",boxShadow:"0 4px 20px 0px rgba(0, 0, 0, 0.14), 0 7px 12px -5px rgba(33, 33, 33, 0.46)"},white:{border:"0",padding:"0.625rem 0",marginBottom:"20px",color:"#555",backgroundColor:"#fff !important",boxShadow:"0 4px 18px 0px rgba(0, 0, 0, 0.12), 0 7px 10px -5px rgba(0, 0, 0, 0.15)"},drawerPaper:Object(p.a)(Object(p.a)({border:"none",bottom:"0",transitionProperty:"top, bottom, width",transitionDuration:".2s, .2s, .35s",transitionTimingFunction:"linear, linear, ease",width:260},{boxShadow:"0 10px 30px -12px rgba(0, 0, 0, 0.42), 0 4px 25px 0px rgba(0, 0, 0, 0.12), 0 8px 10px -5px rgba(0, 0, 0, 0.2)"}),{},{position:"fixed",display:"block",top:"0",height:"100vh",right:"0",left:"auto",visibility:"visible",overflowY:"visible",borderTop:"none",textAlign:"left",paddingRight:"0px",paddingLeft:"0"},{transition:"all 0.33s cubic-bezier(0.685, 0.0473, 0.346, 1)"})},J=Object(c.a)(_);function G(e){var a,t=J(),n=i.a.useState(!1),r=Object(R.a)(n,2),o=r[0],l=r[1];i.a.useEffect((function(){return e.changeColorOnScroll&&window.addEventListener("scroll",c),function(){e.changeColorOnScroll&&window.removeEventListener("scroll",c)}}));var s=function(){l(!o)},c=function(){var a=e.color,n=e.changeColorOnScroll;window.pageYOffset>n.height?(document.body.getElementsByTagName("header")[0].classList.remove(t[a]),document.body.getElementsByTagName("header")[0].classList.add(t[n.color])):(document.body.getElementsByTagName("header")[0].classList.add(t[a]),document.body.getElementsByTagName("header")[0].classList.remove(t[n.color]))},p=e.color,m=e.rightLinks,d=e.leftLinks,g=e.brand,h=e.fixed,u=e.absolute,x=b()((a={},Object(f.a)(a,t.appBar,!0),Object(f.a)(a,t[p],p),Object(f.a)(a,t.absolute,u),Object(f.a)(a,t.fixed,h),a)),E=i.a.createElement(U.a,{to:"/#top",style:{color:"inherit"}},i.a.createElement(S.a,{className:t.title},g));return i.a.createElement(P.a,{className:x},i.a.createElement(H.a,{className:t.container},void 0!==d?E:null,i.a.createElement("div",{className:t.flex},void 0!==d?i.a.createElement(z.a,{smDown:!0,implementation:"css"},d):E),i.a.createElement(z.a,{smDown:!0,implementation:"css"},m),i.a.createElement(z.a,{mdUp:!0},i.a.createElement(B.a,{color:"inherit","aria-label":"open drawer",onClick:s},i.a.createElement(M.a,null)))),i.a.createElement(z.a,{mdUp:!0,implementation:"js"},i.a.createElement(D.a,{variant:"temporary",anchor:"right",open:o,classes:{paper:t.drawerPaper},onClose:s},i.a.createElement("div",{className:t.appResponsive},d,m))))}G.defaultProp={color:"white"};var Q=t(174),$=t(187),q={tooltip:{padding:"10px 15px",minWidth:"130px",color:"#555555",lineHeight:"1.7em",background:"#FFFFFF",border:"none",borderRadius:"3px",boxShadow:"0 8px 10px 1px rgba(0, 0, 0, 0.14), 0 3px 14px 2px rgba(0, 0, 0, 0.12), 0 5px 5px -3px rgba(0, 0, 0, 0.2)",maxWidth:"200px",textAlign:"center",fontFamily:'"Helvetica Neue",Helvetica,Arial,sans-serif',fontSize:"0.875em",fontStyle:"normal",fontWeight:"400",textShadow:"none",textTransform:"none",letterSpacing:"normal",wordBreak:"normal",wordSpacing:"normal",wordWrap:"normal",whiteSpace:"normal",lineBreak:"auto"}},K=function(e){return Object(p.a)(Object(p.a)({list:Object(p.a)(Object(p.a)({},d),{},{fontSize:"14px",margin:0,paddingLeft:"0",listStyle:"none",paddingTop:"0",paddingBottom:"0",color:"inherit"}),listItem:Object(f.a)({float:"left",color:"inherit",position:"relative",display:"block",width:"auto",margin:"0",padding:"0"},e.breakpoints.down("sm"),{width:"100%","&:after":{width:"calc(100% - 30px)",content:'""',display:"block",height:"1px",marginLeft:"15px",backgroundColor:"#e5e5e5"}}),listItemText:{padding:"0 !important"},navLink:Object(f.a)({color:"inherit",position:"relative",padding:"0.9375rem",fontWeight:"400",fontSize:"12px",textTransform:"uppercase",borderRadius:"3px",lineHeight:"20px",textDecoration:"none",margin:"0px",display:"inline-flex","&:hover,&:focus":{color:"inherit",background:"rgba(200, 200, 200, 0.2)"}},e.breakpoints.down("sm"),{width:"calc(100% - 30px)",marginLeft:"15px",marginBottom:"8px",marginTop:"8px",textAlign:"left","& > span:first-child":{justifyContent:"flex-start"}}),notificationNavLink:{color:"inherit",padding:"0.9375rem",fontWeight:"400",fontSize:"12px",textTransform:"uppercase",lineHeight:"20px",textDecoration:"none",margin:"0px",display:"inline-flex",top:"4px"},registerNavLink:{top:"3px",position:"relative",fontWeight:"400",fontSize:"12px",textTransform:"uppercase",lineHeight:"20px",textDecoration:"none",margin:"0px",display:"inline-flex"},navLinkActive:{color:"inherit",backgroundColor:"rgba(255, 255, 255, 0.1)"},icons:{width:"20px",height:"20px",marginRight:"3px"},socialIcons:{position:"relative",fontSize:"20px !important",marginRight:"4px"},dropdownLink:{"&,&:hover,&:focus":{color:"inherit",textDecoration:"none",display:"block",padding:"10px 20px"}}},q),{},{marginRight5:{marginRight:"5px"}})},Y=Object(c.a)(K);function Z(e){var a=Y();return i.a.createElement(Q.a,{className:a.list},i.a.createElement($.a,{className:a.listItem},i.a.createElement(U.a,{to:"/#description",style:{color:"inherit"}},i.a.createElement(A,{color:"transparent",className:a.navLink},"Description"))),i.a.createElement($.a,{className:a.listItem},i.a.createElement(U.a,{to:"/#call",style:{color:"inherit"}},i.a.createElement(A,{color:"transparent",className:a.navLink},"Call for Papers"))),i.a.createElement($.a,{className:a.listItem},i.a.createElement(U.a,{to:"/#deadlines",style:{color:"inherit"}},i.a.createElement(A,{color:"transparent",className:a.navLink},"Deadlines"))),i.a.createElement($.a,{className:a.listItem},i.a.createElement(U.a,{to:"/#schedule",style:{color:"inherit"}},i.a.createElement(A,{color:"transparent",className:a.navLink},"Schedule"))),i.a.createElement($.a,{className:a.listItem},i.a.createElement(U.a,{to:"/#organizers",style:{color:"inherit"}},i.a.createElement(A,{color:"transparent",className:a.navLink},"Organizers"))),i.a.createElement($.a,{className:a.listItem},i.a.createElement(U.a,{to:"/#papers",style:{color:"inherit"}},i.a.createElement(A,{color:"transparent",className:a.navLink},"Accepted Papers"))),i.a.createElement($.a,{className:a.listItem},i.a.createElement(U.a,{to:"/#contact",style:{color:"inherit"}},i.a.createElement(A,{color:"transparent",className:a.navLink},"Contact"))),i.a.createElement($.a,{className:a.listItem},i.a.createElement(U.a,{to:"/#previous_workshops",style:{color:"inherit"}},i.a.createElement(A,{color:"transparent",className:a.navLink},"Previous Workshops"))))}var V={parallax:{height:"100vh",maxHeight:"1000px",overflow:"hidden",position:"relative",backgroundPosition:"center center",backgroundSize:"cover",margin:"0",padding:"0",border:"0",display:"flex",alignItems:"center"},filter:{"&:before":{background:"rgba(0, 0, 0, 0.5)"},"&:after,&:before":{position:"absolute",zIndex:"1",width:"100%",height:"100%",display:"block",left:"0",top:"0",content:"''"}},small:{height:"380px"}},X=Object(c.a)(V);function ee(e){var a,t;t=window.innerWidth>=768?window.pageYOffset/3:0;var n=i.a.useState("translate3d(0,"+t+"px,0)"),r=Object(R.a)(n,2),o=r[0],l=r[1];i.a.useEffect((function(){return window.innerWidth>=768&&window.addEventListener("scroll",s),function(){window.innerWidth>=768&&window.removeEventListener("scroll",s)}}));var s=function(){var e=window.pageYOffset/3;l("translate3d(0,"+e+"px,0)")},c=e.filter,m=e.className,d=e.children,g=e.style,h=e.image,u=e.small,x=X(),E=b()((a={},Object(f.a)(a,x.parallax,!0),Object(f.a)(a,x.filter,c),Object(f.a)(a,x.small,u),Object(f.a)(a,m,void 0!==m),a));return i.a.createElement("div",{className:E,style:Object(p.a)(Object(p.a)({},g),{},{backgroundImage:"url("+h+")",transform:o})},d)}var ae={section:{padding:"0px 0",textAlign:"center"},title:Object(p.a)(Object(p.a)({},g),{},{fontSize:"1.5em",marginBottom:"1rem",marginTop:"30px",minHeight:"32px",textDecoration:"none"}),description:{color:"#696969",margin:"20px 0",textAlign:"left"},paper:{marginTop:"30px",marginBottom:"30px",padding:"10px",textAlign:"left"}},te=Object(c.a)(ae);function ne(){var e=te();return i.a.createElement("div",{className:e.section,style:{textAlign:"left"}},i.a.createElement("h2",{className:e.title},"Call for Papers"),i.a.createElement("h5",{className:e.description},"The workshop welcomes the submission of work on, but not limited to, the following research directions."),i.a.createElement("ul",{style:{color:"#696969",lineHeight:"30px"}},i.a.createElement("li",null,"New self-supervised proxy tasks or new approaches using self-supervised models in speech and audio processing."),i.a.createElement("li",null,"Theoretical or empirical studies focusing on understanding why self-supervision methods work for speech and audio."),i.a.createElement("li",null,"Exploring the limits of self-supervised learning approaches for speech and audio processing, for example, adverse environment conditions, multiple languages, or generalization across downstream tasks."),i.a.createElement("li",null,"Comparison or integration of self-supervised learning methods and other semi-supervised and transfer learning methods in speech and audio processing tasks."),i.a.createElement("li",null,"Self-supervised learning approaches involving the interaction of speech/audio and other modalities.")),i.a.createElement("br",null),i.a.createElement("h5",{className:e.description},"The workshop also welcomes participants of"," ",i.a.createElement("a",{href:"https://superbbenchmark.org/"},"SUPERB")," and"," ",i.a.createElement("a",{href:"https://zerospeech.com/2021/index.html"},"Zero Speech")," ","challenge to submit their results."),i.a.createElement("ul",{style:{color:"#696969",lineHeight:"30px"}},i.a.createElement("li",null,"SUPERB is a benchmarking platform that allows the community to train, evaluate, and compare the speech representations on diverse downstream speech processing tasks. The challenge requires participants to build competitive models for diverse downstream tasks with limited labeled data and trainable parameters, by reusing self-supervised pre-trained networks."),i.a.createElement("li",null,"Zero Speech challenge is to build language models only based on audio or audio-visual information, without using any textual input. The trained models are intended to assign scores to novel utterances, assessing whether they are possible or likely utterances in the training language.")),i.a.createElement("br",null),i.a.createElement("h5",{className:e.description},"Papers may consist of up to eight (8 ) pages of content, plus unlimited pages of references and appendix, in the AAAI submission format and will be submitted to the submission site in the track of regular paper, SUPERB or Zero Speech result paper. Each paper will be reviewed by three reviewers from the workshop program committee in double-blind. Accepted papers will not be archived but will be hosted on the workshop website. We allow papers that are concurrently submitted to or currently under review at other conferences or venues."),i.a.createElement("h5",{className:e.description},"We also want to let you know that"," ",i.a.createElement("a",{href:"https://signalprocessingsociety.org/blog/ieee-jstsp-special-issue-self-supervised-learning-speech-and-audio-processing"},"IEEE JSTSP Special Issue on Self-Supervised Learning for Speech and Audio Processing")," ","is call-for-paper. The deadline is ",i.a.createElement("b",null,"January 15, 2022"),". Submissions to both the workshop and the special issue are possible, but the journal paper should show added extension. If you want to submit to the special issue but have problems meeting the deadline, please feel free to contact Hung-yi Lee (hungyilee@ntu.edu.tw)."))}var re={section:{padding:"10px 0",textAlign:"center"},title:Object(p.a)(Object(p.a)({},g),{},{fontSize:"1.5em",marginBottom:"1rem",marginTop:"30px",minHeight:"32px",textDecoration:"none"}),description:{color:"#696969",margin:"30px 0"}},ie=Object(c.a)(re);function oe(e){var a=ie(),t=e.title,n=e.descriptions;return i.a.createElement("div",{className:a.section,style:{textAlign:"left"}},i.a.createElement("h2",{className:a.title},t),n.map((function(e){return i.a.createElement("h5",{className:a.description},e)})))}var le={section:{padding:"0px 0",textAlign:"center"},title:Object(p.a)(Object(p.a)({},g),{},{fontSize:"1.5em",marginBottom:"1rem",marginTop:"30px",minHeight:"32px",textDecoration:"none"}),description:{color:"#696969",margin:"20px 0",textAlign:"left"},paper:{marginTop:"30px",marginBottom:"30px",padding:"10px",textAlign:"left"}},se=t(89),ce=t(5),pe=t(178),me=t(180),de=t(175),ge=t(177),he=t(179),ue=t(176),xe=Object(ce.a)((function(e){return{head:{backgroundColor:e.palette.primary.light,color:e.palette.common.white},body:{fontSize:14}}}))(de.a),be=Object(ce.a)((function(e){return{root:{"&:nth-of-type(odd)":{backgroundColor:e.palette.action.hover}}}}))(ue.a);function fe(e,a,t){return{time:e,activity:a,title:t}}var Ee=[fe("8:50 - 9:00","Opening","Organizers"),fe("9:00 - 9:50","Keynote 1","40 mins presentation + 10 mins Q&A"),fe("9:50 - 11:00","Invited talks 1 & 2 & 3","20 mins presentations for each speaker + 10 mins Q&A")],ve=[fe("11:10 - 12:00","Paper presentation - session 1","Regular paper (10 mins for each speaker + 10 mins Q&A)"),fe("12:00 - 12:50","Paper presentation - session 2","Regular paper (10 mins for each speaker + 10 mins Q&A)")],we=[fe("13:20 - 14:30","Invited talk 4 & 5 & 6","20 mins presentations for each speaker + 10 mins Q&A"),fe("14:30 - 15:20","Paper presentation - session 3","Regular paper (10 mins for each speaker + 10 mins Q&A)")],Se=[fe("15:50 - 16:40","Keynote 2","40 mins presentation + 10 mins Q&A ")],ke=[fe("17:10 - 18:00","Invited talks 7 & 8","20 mins presentations for each speaker + 10 mins Q&A"),fe("18:00 - 19:00","Paper presentation (SUPERB)","3 papers + wavLM + result presentation (10 mins each) + 10 mins Q&A"),fe("19:00 - 19:10","Closing remarks","Organizers")],ye=Object(c.a)({table:{minWidth:700}});function je(){var e=ye();return i.a.createElement(ge.a,{component:se.a},i.a.createElement(pe.a,{className:e.table,"aria-label":"customized table"},i.a.createElement(he.a,null,i.a.createElement(ue.a,null,i.a.createElement(xe,null,"Time (EST)"),i.a.createElement(xe,{align:"left"},"Activity"),i.a.createElement(xe,{align:"left"},"Presenters / Presentation title / Note"))),i.a.createElement(me.a,null,Ee.map((function(e){return i.a.createElement(be,{key:e.title},i.a.createElement(xe,{align:"left"},e.time),i.a.createElement(xe,{align:"left"},e.activity),i.a.createElement(xe,{align:"left"},e.title))})),i.a.createElement(be,null,i.a.createElement(xe,{colspan:"3",align:"center"},"10-min Break")),ve.map((function(e){return i.a.createElement(be,{key:e.title},i.a.createElement(xe,{align:"left"},e.time),i.a.createElement(xe,{align:"left"},e.activity),i.a.createElement(xe,{align:"left"},e.title))})),i.a.createElement(be,null,i.a.createElement(xe,{colspan:"3",align:"center"},"30-min Break")),we.map((function(e){return i.a.createElement(be,{key:e.title},i.a.createElement(xe,{align:"left"},e.time),i.a.createElement(xe,{align:"left"},e.activity),i.a.createElement(xe,{align:"left"},e.title))})),i.a.createElement(be,null,i.a.createElement(xe,{colspan:"3",align:"center"},"30-min Break")),Se.map((function(e){return i.a.createElement(be,{key:e.title},i.a.createElement(xe,{align:"left"},e.time),i.a.createElement(xe,{align:"left"},e.activity),i.a.createElement(xe,{align:"left"},e.title))})),i.a.createElement(be,null,i.a.createElement(xe,{colspan:"3",align:"center"},"30-min Break")),ke.map((function(e){return i.a.createElement(be,{key:e.title},i.a.createElement(xe,{align:"left"},e.time),i.a.createElement(xe,{align:"left"},e.activity),i.a.createElement(xe,{align:"left"},e.title))})))))}var Ae=Object(c.a)(le);function Ne(e){var a=Ae();return i.a.createElement("div",{className:a.section,style:{textAlign:"left"}},i.a.createElement("h2",{className:a.title},"Schedule"),i.a.createElement("p",{style:{height:10}}),i.a.createElement(je,null),i.a.createElement("h5",{className:a.title},"Paper presentation - session 1"),i.a.createElement("ul",{style:{color:"#696969",lineHeight:"30px"}},i.a.createElement("li",null,"S3PRL-VC: Open-source Voice Conversion Framework with Self-supervised Speech Representations"),i.a.createElement("li",null,"Pronunciation Adaptive Self Speaking Agent Using WaveGrad"),i.a.createElement("li",null,"Mandarin-English Code-Switching Speech Recognition with Self-Supervised Speech Representation Models"),i.a.createElement("li",null,"Don't speak too fast: The impact of data bias on self-supervised speech models")),i.a.createElement("h5",{className:a.title},"Paper presentation - session 2"),i.a.createElement("ul",{style:{color:"#696969",lineHeight:"30px"}},i.a.createElement("li",null,"Characterizing the Adversarial Vulnerability of Speech Self-Supervised Learning"),i.a.createElement("li",null,"Membership Inference Attacks Against Self-supervised Speech Models"),i.a.createElement("li",null,"Pretext Tasks Selection for Multitask Self-Supervised Speech Representation"),i.a.createElement("li",null,"DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning")),i.a.createElement("h5",{className:a.title},"Paper presentation - session 3"),i.a.createElement("ul",{style:{color:"#696969",lineHeight:"30px"}},i.a.createElement("li",null,"An Overview of Unsupervised Representation Learning for Speech, Lasse Borgholt"),i.a.createElement("li",null,"Do self-supervised speech models develop human-like perception biases"),i.a.createElement("li",null,"Investigation on instance mixup regularization strategies for self-supervised speaker representation learning"),i.a.createElement("li",null,"Detecting Depression with a Temporal Context of Speaker Embeddings")),i.a.createElement("h5",{className:a.title},"Paper presentation (SUPERB)"),i.a.createElement("ul",{style:{color:"#696969",lineHeight:"30px"}},i.a.createElement("li",null,"SUPERB introduction and hidden set evaluation results"),i.a.createElement("li",null,"Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling"),i.a.createElement("li",null,"Invitation talk: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing"),i.a.createElement("li",null,"DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT"),i.a.createElement("li",null,"Speech Representation Learning through Self-supervised Pretraining and Multi-task Finetuning")))}var Fe=t(181),Ce=t(182),Oe=t(183),Te=t(22),Le=Object(c.a)((function(e){return{root:{display:"flex"},details:{display:"flex",flexDirection:"column"},content:{flex:"1 0 auto"},cover:{width:151},controls:{display:"flex",alignItems:"center",paddingLeft:e.spacing(1),paddingBottom:e.spacing(1)},playIcon:{height:38,width:38}}}));function Ie(e){var a=Le(),t=(Object(Te.a)(),e.name),n=e.org,r=e.page,o=e.imgurl;return i.a.createElement(Fe.a,{className:a.root,style:{height:150}},i.a.createElement("div",{className:a.details,style:{width:260}},i.a.createElement(Ce.a,{className:a.content},i.a.createElement("h4",{style:{fontSize:18}},t),i.a.createElement("p",{style:{fontSize:13}},i.a.createElement("strong",null,n)),i.a.createElement("p",{style:{fontSize:13}},i.a.createElement("a",{href:r},"visit page")))),i.a.createElement(Oe.a,{className:a.cover,image:o,title:"Live from space album cover"}))}var Re=Object(c.a)(le);function Pe(e){e.name,e.object;var a=Re(),t=[["James Glass","MIT","https://www.csail.mit.edu/person/jim-glass","https://www.csail.mit.edu/sites/default/files/styles/headshot/public/images/people/profile/jim%20glass-full-17%20copy.jpg?h=4c964bae&itok=gF1QBfpQ"],["Kristen Grauman","University of Texas at Austin","https://www.cs.utexas.edu/users/grauman/","https://www.cs.utexas.edu/users/grauman/grauman.jpg"],["Yu Zhang","Google Brain","https://research.google/people/105680/","https://sunprinces.github.io/interspeech2020-meta-learning/authors/yu/avatar_hu8314cdd3c0c87f4902511c1e311fafc1_69482_270x270_fill_q90_lanczos_center.jpg"],["Danqi Chen","Princeton University","https://www.cs.princeton.edu/~danqic/","https://www.cs.princeton.edu/~danqic/images/danqi_2019.jpg"],["Alexei Baevski","FAIR","https://ai.facebook.com/people/alexei-baevski/","https://i.imgur.com/cjrnYUl.png"],["Wei-Ning Hsu","FAIR","https://scholar.google.com/citations?user=N5HDmqoAAAAJ&hl=en","https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=N5HDmqoAAAAJ&citpid=3"],["Herman Kamper","Stellenbosch University","https://www.kamperh.com/","https://www.kamperh.com/images/herman_scaled_rounded.jpg"],["Jan Chorowski","University of Wroclaw and NavAlgo","https://sites.google.com/a/cs.uni.wroc.pl/jch/home/about","https://warsaw.ai/wp-content/uploads/2020/12/chorowski_larger-e1609416203905.jpg"],["Sakriani Sakti","JAIST / NAIST","https://www.jaist.ac.jp/~ssakti/index.html","https://ahcweb01.naist.jp/ssakti/ssakti2.JPG"],["Odette Scharenborg","Delft University of Technology","https://www.tudelft.nl/en/eemcs/the-faculty/departments/intelligent-systems/multimedia-computing/people/odette-scharenborg","https://d2k0ddhflgrk1i.cloudfront.net/_processed_/f/6/csm_IMG_3139-2_f6480d7740.jpg"]];return i.a.createElement("div",{className:a.section,style:{textAlign:"left"}},i.a.createElement("h2",{className:a.title},"Speakers"),i.a.createElement(O,null,i.a.createElement(I,{xs:12,sm:12,md:8},i.a.createElement("h4",{className:a.description,style:{textAlign:"left"}},"Keynote speakers:")),i.a.createElement(I,{xs:12,sm:12,md:8},i.a.createElement(O,null,t.slice(0,2).map((function(e){return i.a.createElement(I,{xs:12,sm:12,md:12,lg:6},i.a.createElement("div",{style:{padding:10,maxWidth:400,margin:"auto"}},i.a.createElement(Ie,{name:e[0],org:e[1],page:e[2],imgurl:e[3]})))})))),i.a.createElement(I,{xs:12,sm:12,md:8},i.a.createElement("h4",{className:a.description,style:{textAlign:"left"}},"Invited speakers:")),i.a.createElement(I,{xs:12,sm:12,md:8},i.a.createElement(O,null,t.slice(2).map((function(e){return i.a.createElement(I,{xs:12,sm:12,md:12,lg:6},i.a.createElement("div",{style:{padding:10,maxWidth:400,margin:"auto"}},i.a.createElement(Ie,{name:e[0],org:e[1],page:e[2],imgurl:e[3]})))}))))),i.a.createElement("p",{style:{height:30}}))}var He={section:{padding:"0px 0",textAlign:"center"},title:Object(p.a)(Object(p.a)({},g),{},{fontSize:"1.5em",marginBottom:"1rem",marginTop:"30px",minHeight:"32px",textDecoration:"none"}),description:{color:"#696969",margin:"20px 0",textAlign:"left"},paper:{marginTop:"30px",marginBottom:"30px",padding:"10px",textAlign:"left"}},Be=Object(c.a)(He);function ze(){var e=Be();return i.a.createElement("div",{className:e.section,style:{textAlign:"left"}},i.a.createElement("h2",{className:e.title},"Workshop Description"),i.a.createElement("h5",{className:e.description},"Babies learn their first language through listening, talking, and interacting with adults. Can AI achieve the same goal without much low-level supervision? Inspired by the question, there is a trend in the machine learning community to adopt self-supervised approaches to pre-train deep networks. Self-supervised learning utilizes proxy supervised learning tasks, for example, distinguishing parts of the input signal from distractors, or generating masked input segments conditioned on the unmasked ones, to obtain training data from unlabeled corpora. These approaches make it possible to use a tremendous amount of unlabeled data available on the web to train large networks and solve complicated tasks. BERT and GPT in NLP and SimCLR and BYOL in CV are famous examples in this direction. Recently self-supervised approaches for speech/audio processing are also gaining attention. There were two workshops on similar topics hosted at ",i.a.createElement("a",{href:"https://icml-sas.gitlab.io/"},"ICML 2020")," and ",i.a.createElement("a",{href:"https://neurips-sas-2020.github.io/"},"NeurIPS 2020"),", and both workshops observed positive feedback and overwhelming participation. We are excited to continue promoting innovation in self-supervision for the speech/audio processing fields and inspiring the fields to contribute to the general machine learning community. The goal of this workshop is to connect researchers in self-supervision inside and outside the speech and audio fields to discuss cutting-edge technology, inspire ideas and collaborations, and drive the research frontier."))}var De={imgFluid:{maxWidth:"100%",height:"auto"},imgRounded:{borderRadius:"6px !important"},imgRoundedCircle:{borderRadius:"50% !important"},imgRaised:{boxShadow:"0 5px 15px -8px rgba(0, 0, 0, 0.24), 0 8px 10px -5px rgba(0, 0, 0, 0.2)"},imgGallery:{width:"100%",marginBottom:"2.142rem"},imgCardTop:{width:"100%",borderTopLeftRadius:"calc(.25rem - 1px)",borderTopRightRadius:"calc(.25rem - 1px)"},imgCardBottom:{width:"100%",borderBottomLeftRadius:"calc(.25rem - 1px)",borderBottomRightRadius:"calc(.25rem - 1px)"},imgCard:{width:"100%",borderRadius:"calc(.25rem - 1px)"},imgCardOverlay:{position:"absolute",top:"0",right:"0",bottom:"0",left:"0",padding:"1.25rem"}},We=Object(p.a)(Object(p.a)({section:{padding:"0px 0",textAlign:"center"},title:Object(p.a)(Object(p.a)({},g),{},{fontSize:"1.5em",marginBottom:"1rem",marginTop:"30px",minHeight:"32px",textDecoration:"none"})},De),{},{itemGrid:{marginLeft:"auto",marginRight:"auto"},cardTitle:h,smallTitle:{color:"#6c757d"},description:{color:"#696969",maxHeight:"7em",overflow:"hidden",paddingBottom:"20px"},justifyCenter:{justifyContent:"center !important"},socials:{marginTop:"0",width:"100%",transform:"none",left:"0",top:"0",height:"100%",lineHeight:"41px",fontSize:"20px",color:"#696969"},margin5:{margin:"5px"}}),Me={card:{border:"0",marginBottom:"30px",marginTop:"30px",borderRadius:"6px",color:"rgba(0, 0, 0, 0.87)",background:"#fff",width:"100%",boxShadow:"0 2px 2px 0 rgba(0, 0, 0, 0.14), 0 3px 1px -2px rgba(0, 0, 0, 0.2), 0 1px 5px 0 rgba(0, 0, 0, 0.12)",position:"relative",display:"flex",flexDirection:"column",minWidth:"0",wordWrap:"break-word",fontSize:".875rem",transition:"all 300ms linear"},cardPlain:{background:"transparent",boxShadow:"none"},cardCarousel:{overflow:"hidden"}},Ue=["className","children","plain","carousel"],_e=Object(c.a)(Me);function Je(e){var a,t=_e(),n=e.className,r=e.children,o=e.plain,l=e.carousel,s=Object(E.a)(e,Ue),c=b()((a={},Object(f.a)(a,t.card,!0),Object(f.a)(a,t.cardPlain,o),Object(f.a)(a,t.cardCarousel,l),Object(f.a)(a,n,void 0!==n),a));return i.a.createElement("div",Object.assign({className:c},s),r)}var Ge={cardBody:{padding:"0.9375rem 1.875rem",flex:"1 1 auto"}},Qe=["className","children"],$e=Object(c.a)(Ge);function qe(e){var a,t=$e(),n=e.className,r=e.children,o=Object(E.a)(e,Qe),l=b()((a={},Object(f.a)(a,t.cardBody,!0),Object(f.a)(a,n,void 0!==n),a));return i.a.createElement("div",Object.assign({className:l},o),r)}var Ke=t(184),Ye=t(173),Ze=t(139),Ve=t(188),Xe=Object(c.a)((function(e){return{modal:{display:"flex",alignItems:"center",justifyContent:"center"},paper:{backgroundColor:e.palette.background.paper,border:"2px solid #000",boxShadow:e.shadows[5],padding:e.spacing(2,4,3)}}}));function ea(e){var a=e.page,t=e.description,n=Xe(),r=i.a.useState(!1),o=Object(R.a)(r,2),l=o[0],s=o[1];return i.a.createElement("div",null,i.a.createElement(Ve.a,{onClick:function(){s(!0)}},"... read more"),i.a.createElement("span",null," or "),i.a.createElement("a",{href:a,target:"_blank"},"visit page"),i.a.createElement(Ke.a,{"aria-labelledby":"transition-modal-title","aria-describedby":"transition-modal-description",className:n.modal,open:l,onClose:function(){s(!1)},closeAfterTransition:!0,BackdropComponent:Ye.a,BackdropProps:{timeout:500}},i.a.createElement(Ze.a,{in:l},i.a.createElement(se.a,{style:{maxWidth:500,padding:20}},i.a.createElement("p",null,t)))))}var aa=Object(c.a)(We);function ta(e){var a=e.classes,t=e.imageClasses,n=e.name,r=e.email,o=e.title,l=e.description,s=e.imgurl,c=e.page;return i.a.createElement(I,{xs:12,sm:12,md:4},i.a.createElement(Je,{plain:!0},i.a.createElement(I,{xs:12,sm:12,md:6,className:a.itemGrid},i.a.createElement("img",{src:s,alt:"...",className:t,style:{width:150,height:150,objectFit:"cover"}})),i.a.createElement("h4",{className:a.cardTitle,style:{textAlign:"center"}},n,i.a.createElement("br",null),i.a.createElement("small",{className:a.smallTitle},o),i.a.createElement("br",null),i.a.createElement("small",{className:a.smallTitle},r)),i.a.createElement(qe,null,i.a.createElement("p",{className:a.description},l),i.a.createElement(ea,{page:c,description:l}))))}function na(){var e=aa(),a=b()(e.imgRaised,e.imgRoundedCircle,e.imgFluid),t=[["Abdelrahman Mohamed","Facebook","abdo@fb.com",i.a.createElement("span",null,i.a.createElement("strong",null,"Abdelrahman Mohamed"),' is a research scientist at Facebook AI research (FAIR) in Seattle. Before FAIR, he was a principal scientist/manager in Amazon Alexa AI team. From 2014 to 2017, he was in Microsoft Research Redmond. He received his PhD from the University of Toronto with Geoffrey Hinton and Gerald Penn where he was part of the team that started the Deep Learning revolution in Spoken Language Processing in 2009. He is the recipient of the IEEE Signal Processing Society Best Journal Paper Award for 2016. His research interests span Deep Learning, Spoken Language Processing, and Natural Language Understanding. He is the co-organizer of the special session on "New Trends in self-supervised speech processing" at Interspeech (2020) and two workshops "Self-Supervised Learning for Speech and Audio Processing", and \u201cSelf-Supervised Learning - Theory and Practice\u201d at NeurIPS (2020).'),"https://www.cs.toronto.edu/~asamir/Abdelrahman_Mohamed.jpg","https://research.fb.com/people/mohamed-abdelrahman/"],["Hung-yi Lee","National Taiwan University","hungyilee@ntu.edu.tw",i.a.createElement("span",null,i.a.createElement("strong",null,"Hung-yi Lee"),' received the M.S. and Ph.D. degrees from National Taiwan University (NTU), Taipei, Taiwan, in 2010 and 2012, respectively. From September 2012 to August 2013, he was a postdoctoral fellow in Research Center for Information Technology Innovation, Academia Sinica. From September 2013 to July 2014, he was a visiting scientist at the Spoken Language Systems Group of MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). He is currently an associate professor of the Department of Electrical Engineering of National Taiwan University, with a joint appointment at the Department of Computer Science & Information Engineering of the university. His research focuses on machine learning (especially deep learning), spoken language understanding and speech recognition. He is the special session co-organizer about "Meta Learning for Human Language Technology" and \u201cNew Trends in self-supervised speech processing\u201d at Interspeech (2020).'),"https://yt3.ggpht.com/a/AATXAJyD-nikDxFYMS26xENSwBrdycxguBTdocSATQ=s900-c-k-c0xffffffff-no-rj-mo","https://speech.ee.ntu.edu.tw/~tlkagk/"],["Shinji Watanabe","Carnegie Mellon University","shinjiw@ieee.org",i.a.createElement("span",null,i.a.createElement("strong",null,"Shinji Watanabe")," is an Associate Professor at Carnegie Mellon University, Pittsburgh, PA. He received his B.S., M.S., and Ph.D. (Dr. Eng.) degrees from Waseda University, Tokyo, Japan. He was a research scientist at NTT Communication Science Laboratories, Kyoto, Japan, from 2001 to 2011, a visiting scholar in Georgia institute of technology, Atlanta, GA in 2009, and a senior principal research scientist at Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA USA from 2012 to 2017. Prior to the move to Carnegie Mellon University, he was an associate research professor at Johns Hopkins University, Baltimore, MD USA from 2017 to 2020. His research interests include automatic speech recognition, speech enhancement, spoken language understanding, and machine learning for speech and language processing. He has published more than 200 papers in peer-reviewed journals and conferences and received several awards, including the best paper award from the IEEE ASRU in 2019. He served as an Associate Editor of the IEEE Transactions on Audio Speech and Language Processing. He was/has been a member of several technical committees, including the APSIPA Speech, Language, and Audio Technical Committee (SLA), IEEE Signal Processing Society Speech and Language Technical Committee (SLTC), and Machine Learning for Signal Processing Technical Committee (MLSP)."),"https://i.imgur.com/QZk6wXw.png","https://sites.google.com/view/shinjiwatanabe"],["Tara Sainath","Google","tsainath@google.com",i.a.createElement("span",null,i.a.createElement("strong",null,"Tara Sainath"),' received her PhD in Electrical Engineering and Computer Science from MIT in 2009. The main focus of her PhD work was in acoustic modeling for noise robust speech recognition. After her PhD, she spent 5 years at the Speech and Language Algorithms group at IBM T.J. Watson Research Center, before joining Google Research. She has co-organized a special session on Sparse Representations at Interspeech 2010 in Japan. She has also organized a special session on Deep Learning at ICML 2013 in Atlanta, a special session on "New Trends in self-supervised speech processing" at Interspeech (2020), and the workshop on "Self-Supervised Learning for Speech and Audio Processing" at NeurIPS (2020). In addition, she is a staff reporter for the IEEE Speech and Language Processing Technical Committee (SLTC) Newsletter. Her research interests are mainly in acoustic modeling, including deep neural networks, sparse representations and adaptation methods.'),"https://i.imgur.com/Nq2TItJ.png","https://ai.google/research/people/TaraSainath/"],["Karen Livescu","TTI-Chicago","klivescu@ttic.edu",i.a.createElement("span",null,i.a.createElement("strong",null,"Karen Livescu")," is an Associate Professor at TTI-Chicago, a philanthropically endowed academic computer science institute located on the University of Chicago campus. She completed her PhD in 2005 at MIT in the Spoken Language Systems group of the Computer Science and Artificial Intelligence Laboratory. In 2005-2007 she was a post-doctoral lecturer in the MIT EECS department. Her main research interests are in speech and language processing and related problems in machine learning. Her recent work includes multi-view representation learning, acoustic word embeddings, visually grounded speech modeling, and automatic sign language recognition. Her recent professional activities include serving as a program chair of ICLR 2019 and a technical co-chair of ASRU 2015/2017/2019 and Interspeech 2022."),"https://www.ttic.edu/img/livescu.jpg","https://ttic.uchicago.edu/~klivescu/"],["Shang-Wen Li","Facebook","shangwel@fb.com",i.a.createElement("span",null,i.a.createElement("strong",null,"Shang-Wen Li"),' is an Research and Engineering Manager at Facebook AI, and he worked at Apple Siri, Amazon Alexa and AWS before joining Facebook. He earned his Ph.D. from MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) in 2016. His research is focused on spoken language understanding, dialog management, machine reading comprehension, and low-resource speech processing. He co-organized the workshop of "Self-Supervised Learning for Speech and Audio Processing" at NeurIPS (2020) and the workshop of "Meta Learning and Its Applications to Natural Language Processing" at ACL (2021).'),"https://sunprinces.github.io/interspeech2020-meta-learning/authors/shangwen/avatar_hube98b48111b6c1a4c81f443802127d8f_68512_270x270_fill_q90_lanczos_center.jpg","https://swdanielli.github.io/index.html"],["Ewan Dunbar","University of Toronto","ewan.dunbar@utoronto.ca",i.a.createElement("span",null,i.a.createElement("strong",null,"Ewan Dunbar")," is an Assistant Professor at University of Toronto and affiliated scientist in the Cognitive Machine Learning (CoML) team at the \xc9cole Normale Sup\xe9rieure. He received his PhD from the University of Maryland, College Park, in 2013. His research interests are in psycholinguistics, human speech perception, and language acquisition. He has done previous work in the area of unsupervised speech representation modelling, and has served on the organization committee of the Zero Resource Speech Challenge since 2017."),"https://i.imgur.com/t5x6a2k.png","https://ewan.website/"],["Emmanuel Dupoux","EHESS/Facebook","dpx@fb.com",i.a.createElement("span",null,i.a.createElement("strong",null,"Emmanuel Dupoux")," is full professor at the Ecole des Hautes Etudes en Sciences Sociales (EHESS) and research scientist at Facebook AI Research. He directs the Cognitive Machine Learning (CoML) team at the \xc9cole Normale Sup\xe9rieure (ENS) in Paris and INRIA (www.syntheticlearner.com). His education includes a PhD in Cognitive Science (EHESS), a MA in Computer Science (Orsay University) and a BA in Applied Mathematics (Pierre & Marie Curie University, ENS). His research mixes developmental science, cognitive neuroscience, and machine learning, with a focus on the reverse engineering of infant language and cognitive development. He is an organizer of the Intuitive Physics Benchmark (2017), and led in 2017 a Jelinek Summer Workshop at CMU on multimodal speech learning. He has authored 150 articles in peer reviewed outlets from both cognitive science and language technology. He is the main initiator of the Zero Resource Speech Challenge series, and has served on the organization committee since 2015."),"https://i.imgur.com/2NEDD8C.jpg","http://www.lscp.net/persons/dupoux/"]];return i.a.createElement("div",{className:e.section,style:{textAlign:"left"}},i.a.createElement("h2",{className:e.title},"Organizers"),i.a.createElement("div",null,i.a.createElement(O,null,t.map((function(t){return i.a.createElement(ta,{classes:e,imageClasses:a,name:t[0],title:t[1],email:t[2],description:t[3],imgurl:t[4],page:t[5]})})))))}var ra=[],ia=Object(c.a)(u);var oa=Object(n.a)();l.a.render(i.a.createElement(s.b,{history:oa},i.a.createElement(s.c,null,i.a.createElement(s.a,{path:"/",component:function(e){var a=ia(),n=Object.assign({},e);return i.a.createElement("div",null,i.a.createElement("span",{id:"top"}),i.a.createElement(G,Object.assign({color:"transparent",routes:ra,brand:"AAAI SAS 2022",rightLinks:i.a.createElement(Z,null),fixed:!0,changeColorOnScroll:{height:0,color:"white"}},n)),i.a.createElement(ee,{filter:!0,image:t(133)},i.a.createElement("div",{style:{paddingBottom:"30px"},className:a.container},i.a.createElement(O,null,i.a.createElement(I,{xs:12,sm:12,md:12},i.a.createElement("h2",{className:a.subtitle},"AAAI 2022 workshop"),i.a.createElement("h1",{className:a.title,style:{maxWidth:900}},"The 2nd Workshop on Self-supervised Learning for Audio and Speech Processing"),i.a.createElement("br",null),i.a.createElement(A,{color:"danger",size:"lg",href:"https://cmt3.research.microsoft.com/SAS2022",target:"_blank",rel:"noopener noreferrer"},"SUBMIT PAPERS"))))),i.a.createElement("div",{className:b()(a.main,a.mainRaised)},i.a.createElement("div",{className:a.container},i.a.createElement("span",{style:{display:"block",height:50},id:"description"}),i.a.createElement(ze,null),i.a.createElement("span",{style:{display:"block",height:50},id:"call"}),i.a.createElement(ne,null),i.a.createElement("span",{style:{display:"block",height:50},id:"deadlines"}),i.a.createElement(oe,{title:"Important Dates",descriptions:[i.a.createElement("div",{style:{textAlign:"left"}},i.a.createElement("div",{style:{margin:"30px 0 5px 0"}},i.a.createElement("span",{style:{fontWeight:"bold",color:"red"}},i.a.createElement("s",null,"23:59 (Anywhere on Earth), Nov 12, 2021")),i.a.createElement("br",null),i.a.createElement("span",{style:{fontWeight:"bold",color:"red"}},i.a.createElement("s",null,"23:59 (Anywhere on Earth), Nov 15, 2021")),i.a.createElement("br",null),i.a.createElement("span",{style:{fontWeight:"bold",color:"red"}},"23:59 (Anywhere on Earth), Nov 16, 2021")),i.a.createElement("block",{style:{margin:"5px 0 30px 0"}},"Submission deadline"),i.a.createElement("div",{style:{margin:"30px 0 5px 0"}},i.a.createElement("span",{style:{fontWeight:"bold",color:"red"}},"23:59 (Anywhere on Earth), Dec 3, 2021")),i.a.createElement("block",{style:{margin:"5px 0 30px 0"}},"Notification of Acceptance/Rejection"),i.a.createElement("div",{style:{fontWeight:"bold",color:"red",margin:"30px 0 5px 0"}},"Feb 28, 2022"),i.a.createElement("block",{style:{margin:"5px 0 30px 0"}},"Date of workshop"),i.a.createElement("div",{style:{fontWeight:"bold",color:"red",margin:"30px 0 5px 0"}}),i.a.createElement("block",{style:{margin:"5px 0 30px 0"}},"(Please check"," ",i.a.createElement("a",{href:"https://superbbenchmark.org/"},"SUPERB")," and"," ",i.a.createElement("a",{href:"https://zerospeech.com/2021/index.html"},"Zero Speech")," ","website for challenge-specific timelines)"))]}),i.a.createElement("span",{style:{display:"block",height:50},id:"speakers"}),i.a.createElement(Pe,null),i.a.createElement("span",{style:{display:"block",height:50},id:"schedule"}),i.a.createElement(Ne,null),i.a.createElement("span",{style:{display:"block",height:50},id:"organizers"}),i.a.createElement(na,null),i.a.createElement("span",{style:{display:"block",height:50},id:"papers"}),i.a.createElement("span",{style:{display:"block",height:50},id:"committee"}),i.a.createElement(oe,{title:"Program Committee",descriptions:[i.a.createElement("div",{style:{textAlign:"left"}},[["TBD","TBD"]].map((function(e){return i.a.createElement("div",null,i.a.createElement("div",{style:{margin:"30px 0 5px 0"}},i.a.createElement("span",{style:{fontWeight:"bold"}},e[0])),i.a.createElement("block",{style:{margin:"5px 0 30px 0"}},e[1]))})))]}),i.a.createElement("span",{style:{display:"block",height:50},id:"contact"}),i.a.createElement(oe,{title:"Contact",descriptions:[i.a.createElement("span",{style:{textAlign:"left"}},"sas.aaai.2022@gmail.com")]}),i.a.createElement("span",{style:{display:"block",height:50},id:"previous_workshops"}),i.a.createElement(oe,{title:"Previous Workshops",descriptions:[i.a.createElement("span",{style:{textAlign:"left"}},i.a.createElement("a",{href:"https://neurips-sas-2020.github.io/"},"NeurIPS SAS 2020"))]}),i.a.createElement("p",{style:{height:50}}))),i.a.createElement("p",{style:{height:50}}))}}))),document.getElementById("root"))},99:function(e,a,t){e.exports=t(134)}},[[99,1,2]]]);
//# sourceMappingURL=main.9bb2302b.chunk.js.map